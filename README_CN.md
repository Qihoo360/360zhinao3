<div align="center">
<h1>
  360Zhinao3 (360æ™ºè„‘)
</h1>
</div>
<div align="center">
    ğŸ¤— <a href="https://huggingface.co/qihoo360">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp
    ğŸ’¬ <a href="./WeChat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp
</div>
<br>
<p align="center">
 æ¬¢è¿è®¿é—®360æ™ºè„‘å®˜ç½‘<a href="https://ai.360.com"> https://ai.360.com </a>ä½“éªŒæ›´å¤šæ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚
</p>

<br>

# æ¨¡å‹ä»‹ç»
 ğŸ‰ğŸ‰ğŸ‰**è¿‘æ—¥ï¼Œå¥‡è™360å¼€æºå‡çº§äº†è‡ªç ”çš„7Bå‚æ•°æ¨¡å‹360Zhinao3-7Bï¼Œç°å·²ä¸Šçº¿Githubå¼€æºç¤¾åŒº [**360zhinao3**](https://github.com/Qihoo360/360zhinao3)ï¼Œå¯å…è´¹å•†ç”¨ã€‚æ¨¡å‹å„é¡¹èƒ½åŠ›å¾—åˆ°å…¨é¢æå‡ï¼Œä¸10Bä»¥ä¸‹çš„å°å‚æ•°é‡æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼Œ360Zhinao3-7Båœ¨å¤šä¸ªbenchmarkä¸Šï¼Œå‡å–å¾—ç¬¬ä¸€çš„ä¼˜å¼‚æˆç»©ã€‚**

å¼€æºæ¨¡å‹åŒ…æ‹¬ï¼š
 - **360Zhinao3-7B**
 - **360Zhinao3-7B-Instruct**
 - **360Zhinao3-7B-O1.5**

360Zhinao3-7B æ˜¯åœ¨ 360Zhinao2-7B çš„åŸºç¡€ä¸Šç»§ç»­é¢„è®­ç»ƒäº†700Bçš„é«˜è´¨é‡tokenï¼Œä¸¤è€…æ¨¡å‹ç»“æ„å®Œå…¨ç›¸åŒã€‚æ¨¡å‹æ•ˆæœä¸Šçš„æå‡ï¼Œä¸»è¦æºäºè®­ç»ƒæ•°æ®çš„è´¨é‡çš„æå‡ã€‚

<br>

# æ›´æ–°ä¿¡æ¯
- [2025.04.14] ğŸ”¥ğŸ”¥ğŸ”¥**æˆ‘ä»¬å‘å¸ƒäº†360Zhinao3ç³»åˆ—æ¨¡å‹ï¼ŒåŒæ—¶å¼€æ”¾ 360Zhinao3-7Bã€360Zhinao3-7B-Instruct ä»¥åŠé•¿æ€ç»´é“¾æ¨¡å‹ 360Zhinao3-7B-O1.5ã€‚**
- [2024.11.18] æˆ‘ä»¬å‘å¸ƒäº†360Zhinao2-7Bï¼ŒåŒæ—¶å¼€æ”¾Baseæ¨¡å‹å’Œ4Kã€32Kã€360Kä¸‰ç§æ–‡æœ¬é•¿åº¦çš„Chatæ¨¡å‹ã€‚
- [2024.05.23] æˆ‘ä»¬å‘å¸ƒäº†360Zhinao-searchä»¥åŠ360Zhinao-1.8B-Rerankingä¸¤ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«åœ¨[C-MTEB æ¦œå•](https://huggingface.co/spaces/mteb/leaderboard)çš„Retrievalå’ŒRerankingä»»åŠ¡ä¸Šæ’åç¬¬ä¸€ã€‚
- [2024.05.20] æˆ‘ä»¬å°†llama3çš„çª—å£é•¿åº¦æ‰©å±•åˆ°360kå¹¶å‘å¸ƒäº†**llama3-8B-360Zhinao-360k-Instruct**<a href="https://huggingface.co/qihoo360/llama3-8B-360Zhinao-360k-Instruct">ğŸ¤—</a>
- [2024.04.12] æˆ‘ä»¬å‘å¸ƒäº†360Zhinao-7B 1.0ç‰ˆæœ¬ï¼ŒåŒæ—¶å¼€æ”¾Baseæ¨¡å‹å’Œ4Kã€32Kã€360Kä¸‰ç§æ–‡æœ¬é•¿åº¦çš„Chatæ¨¡å‹ã€‚
æŠ€æœ¯æŠ¥å‘Šè¯¦è§[arXiv](https://arxiv.org/abs/2405.13386)ã€‚

<br>

# ç›®å½•
- [ä¸‹è½½åœ°å€](#ä¸‹è½½åœ°å€)
- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
- [æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)
- [è®¸å¯è¯](#è®¸å¯è¯)

<br>

# ä¸‹è½½åœ°å€
æœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥è§ä¸‹è¡¨ï¼š
| Size | Model | BF16 |
|:-:|-|:-:|
| 7B | 360Zhinao3-7B | <a href="https://huggingface.co/qihoo360/360Zhinao3-7B">ğŸ¤—</a> |
| 7B | 360Zhinao3-7B-Instruct | <a href="https://huggingface.co/qihoo360/360Zhinao3-7B-Instruct">ğŸ¤—</a> |
| 7B | 360Zhinao3-7B-O1.5 | <a href="https://huggingface.co/qihoo360/360Zhinao3-7B-O1.5">ğŸ¤—</a> |

<br>

# æ¨¡å‹è¯„ä¼°

## åŸºç¡€æ¨¡å‹æ•ˆæœ

æˆ‘ä»¬ä½¿ç”¨äº†å¼€æºå·¥å…·opencompasså¯¹æ¨¡å‹è¿›è¡Œå¤šç»´åº¦è¯„ä¼°ï¼Œæ¨¡å‹çš„benchmarkå¹³å‡åˆ†åœ¨10Bä»¥ä¸‹æ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼ŒåŒå°ºå¯¸å…·å¤‡ç«äº‰åŠ›ã€‚

<table>
	<tr>
	    <td>Type</td><td>Datasets</td><td>language</td><td>glm4-9b</td><td>Qwen2.5-7B</td><td>internlm2.5-7b</td><td>Yi1.5-9B</td><td>gemma2-9b</td><td>Llama3.1-8B</td><td>360Zhinao2-7B</td><td>360Zhinao3-7B</td>
	</tr>
	<tr>
	    <td rowspan="5">Exam</td><td>ceval</td><td>zh</td><td>75.83</td><td>81.41</td><td>77.71</td><td>73.51</td><td>56.36</td><td>51.67</td><td>83.04</td><td><strong>84.7</strong></td>
	</tr>
    <tr>
        <td>mmlu</td><td>en</td><td>75.5</td><td>75.5</td><td>71.55</td><td>71.43</td><td>72.22</td><td>66.75</td><td>67.84</td><td>75.42</td>
    </tr>
    <tr>
        <td>cmmlu</td><td>zh</td><td>74.24</td><td>81.79</td><td>78.77</td><td>74.2</td><td>58.89</td><td>52.49</td><td>73.8</td><td><strong>82.17</strong></td>
    </tr>
    <tr>
        <td>ARC-c</td><td>en</td><td>94.92</td><td>80</td><td>85.08</td><td>87.46</td><td>77.63</td><td>80.68</td><td>87.12</td><td>88.14</td>
    </tr>
    <tr>
        <td>ARC-e</td><td>en</td><td>98.41</td><td>84.83</td><td>95.24</td><td>94.53</td><td>78.84</td><td>89.77</td><td>92.77</td><td>94</td>
    </tr>
    <tr>
        <td rowspan="2">Language</td><td>WiC</td><td>en</td><td>51.57</td><td>52.82</td><td>50.78</td><td>50.63</td><td>50.47</td><td>50</td><td>49.84</td><td>50.31</td>
    </tr>
    <tr>
        <td>WSC</td><td>en</td><td>68.27</td><td>68.27</td><td>69.23</td><td>66.35</td><td>68.27</td><td>67.31</td><td>65.38</td><td><strong>71.15</strong></td>
    </tr>
    <tr>
        <td rowspan="2">Knowledge</td>
        <td>BoolQ</td><td>en</td><td>81.8</td><td>83.88</td><td>89.51</td><td>84.46</td><td>85.6</td><td>82.2</td><td>88.29</td><td>88.38</td>
    </tr>
    <tr>
        <td>commonsense_qa</td><td>en</td><td>71.17</td><td>73.22</td><td>68.55</td><td>71.58</td><td>68.47</td><td>71.25</td><td>69.78</td><td>71.33</td>
    </tr>
    <tr>
        <td rowspan="6">Understanding</td>
        <td>C3</td><td>zh</td><td>91.51</td><td>92</td><td>93.04</td><td>85.86</td><td>81.64</td><td>83.51</td><td>93.26</td><td>92.77</td>
    </tr>
    <tr>
        <td>race-middle</td><td>en</td><td>91.99</td><td>91.02</td><td>92.06</td><td>91.16</td><td>88.09</td><td>81.69</td><td>90.46</td><td>90.04</td>
    </tr>
    <tr>
        <td>race-high</td><td>en</td><td>90.71</td><td>87.91</td><td>90.08</td><td>88.34</td><td>82.08</td><td>78.73</td><td>86.74</td><td>85.96</td>
    </tr>
    <tr>
        <td>lcsts</td><td>zh</td><td>18.29</td><td>15.82</td><td>15.96</td><td>16.49</td><td>10.62</td><td>17.29</td><td>18.61</td><td><strong>18.85</strong></td>
    </tr>
    <tr>
        <td>eprstmt-dev</td><td>zh</td><td>91.88</td><td>86.88</td><td>91.25</td><td>91.88</td><td>48.12</td><td>83.12</td><td>90</td><td><strong>92.50</strong></td>
    </tr>
    <tr>
        <td>lambada</td><td>en</td><td>71.67</td><td>71.14</td><td>69.98</td><td>70.64</td><td>75.43</td><td>74.23</td><td>72.56</td><td>68.17</td>
    </tr>
    <tr>
        <td rowspan="3">Reasoning</td>
        <td>hellaswag</td><td>en</td><td>70.25</td><td>72.76</td><td>70.38</td><td>71.55</td><td>66.83</td><td>74.65</td><td>71.49</td><td>73.61</td>
    </tr>
    <tr>
        <td>siqa</td><td>en</td><td>81.73</td><td>72.52</td><td>78.97</td><td>76.2</td><td>58.96</td><td>64.18</td><td>77.12</td><td>79.02</td>
    </tr>
    <tr>
        <td>bbh</td><td>en</td><td>73.68</td><td>54.63</td><td>59.43</td><td>67.86</td><td>68.45</td><td>59.9</td><td>46.54</td><td><strong>73.74</strong></td>
    </tr>
    <tr>
        <td rowspan="2">Code</td>
        <td>humaneval</td><td>en</td><td>69.51</td><td>75</td><td>60.37</td><td>26.22</td><td>5.49</td><td>27.44</td><td>60.98</td><td>64.63</td>
    </tr>
    <tr>
        <td>mbpp</td><td>en</td><td>60</td><td>60</td><td>43.6</td><td>56.8</td><td>51.2</td><td>42.6</td><td>54</td><td><strong>67.80</strong></td>
    </tr>
    <tr>
        <td rowspan="2">Math</td>
        <td>math</td><td>en</td><td>26.86</td><td>38</td><td>27.14</td><td>27.06</td><td>28.52</td><td>15.32</td><td>38.34</td><td>37.60</td>
    </tr>
    <tr>
        <td>gsm8k</td><td>en</td><td>78.54</td><td>79.76</td><td>52.54</td><td>71.11</td><td>73.09</td><td>56.25</td><td>75.51</td><td>78.77</td>
    </tr>
    <tr>
        <td rowspan="2">Overall</td>
        <td>avg_zh</td><td></td><td>70.35</td><td>71.58</td><td>71.35</td><td>68.39</td><td>51.13</td><td>57.62</td><td>71.74</td><td><strong>74.20</strong></td>
    </tr>
    <tr>
        <td>avg_all</td><td></td><td>73.11</td><td>71.78</td><td>69.60</td><td>68.88</td><td>61.60</td><td>62.32</td><td>70.61</td><td><strong>74.83</strong></td>
    </tr>
</table>


## Instructæ¨¡å‹æ•ˆæœ

æˆ‘ä»¬åœ¨IFEvalã€MT-benchã€CF-Benchä¸‰ä¸ªæµè¡Œçš„è¯„æµ‹ä¸Šå¯¹360Zhinao3-7B-Instructæ¨¡å‹è¿›è¡Œäº†è¯„æµ‹æ¯”è¾ƒï¼ŒMT-bench å’ŒCFBenchå‡åœ¨åŒçº§åˆ«å¼€æºæ¨¡å‹ä¸­æ’åç¬¬ä¸€ï¼Œå…·å¤‡è¾ƒå¼ºç«äº‰åŠ›ã€‚åœ¨IFEval (prompt strict) ä»…æ¬¡äºglm4-9bï¼Œåœ¨7Bå°ºå¯¸ä¸Šå¾—åˆ†æœ€é«˜:

| Model                 | MT-bench | IFEval(strict prompt) | CFBench(CSR,ISR,PSR) |          |          |
|-----------------------|----------|-----------------------|----------------------|----------|----------|
| Qwen2.5-7B-Instruct   | 8.07     | 0.556                 | 0.81                 | 0.46     | 0.57     |
| Yi-9B-16k-Chat        | 7.44     | 0.455                 | 0.75                 | 0.4      | 0.52     |
| GLM4-9B-Chat          | 8.08     | **0.634**             | 0.82                 | 0.48     | 0.61     |
| InternLM2.5-7B-Chat   | 7.39     | 0.540                 | 0.78                 | 0.4      | 0.54     |
| 360Zhinao2-7B-Chat-4k | 7.86     | 0.577                 | 0.8                  | 0.44     | 0.57     |
| 360Zhinao3-7B-Instruct| **8.17** | 0.626                 | **0.83**             | **0.52** | **0.64** |

## é•¿æ€ç»´é“¾æ¨¡å‹æ•ˆæœ
æˆ‘ä»¬ç”¨ä¹‹å‰æ™ºè„‘å¼€æºçš„ [Light-R1](https://github.com/Qihoo360/Light-R1) æ–¹æ³•å¯¹360Zhinao3-7B-Instructè¿›è¡Œäº†é•¿æ€ç»´é“¾çš„ç»§ç»­å¾®è°ƒå’ŒRFTï¼ŒGRPOã€‚ä¸æœ€æ–°çš„OpenThinker2-7Bæœ‰ä¸€å®šå·®è·ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ‰€æœ‰ä»¥é€šç”¨Qwen2.5-7B-Instructä¸ºåŸºåº§çš„æ¨¡å‹ã€‚

| Model | Date | Base Model | AIME24 | AIME25 | GPQA Diamond |
| ---- | ---- | ---- | ---- | ---- | ---- |
| OpenThinker2-7B | 25.4.3 | Qwen2.5-7B-Instruct | 50 | 33.3 | 49.3 |
| OpenThinker-7B | 25.1.28 | Qwen2.5-7B-Instruct | 31.3 | 23.3 | 42.4 |
| 360Zhinao3-7B-O1.5 | 25.4.14 | 360Zhinao3-7B-Instruct | 54.2 | 36.3 | 40.0 |
| OpenR1-Qwen-7B | 25.2.11 | Qwen2.5-Math-7B-Instruct | 48.7 | 34.7 | 21.2 |
| DeepSeek-R1-Distill-Qwen-7B | 25.1.20 | Qwen2.5-Math-7B-Instruct | 57.3 | 33.3 | 47.3 |
| Light-R1-7B-DS | 25.3.12 | DeepSeek-R1-Distill-Qwen-7B | 59.1 | 44.3 | 49.4 |
| Areal-boba-RL-7B | 25.3.31 | DeepSeek-R1-Distill-Qwen-7B | 61.9 | 48.3 | 47.6 |


# å¿«é€Ÿå¼€å§‹
ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤—Transformers å¿«é€Ÿä½¿ç”¨ 360Zhinao3-7Bã€360Zhinao3-7B-Instruct ä»¥åŠ 360Zhinao3-7B-O1.5

## ğŸ¤—Transformers
### Baseæ¨¡å‹æ¨ç†

æ­¤ä»£ç æ¼”ç¤ºä½¿ç”¨transformerså¿«é€Ÿä½¿ç”¨360Zhinao2-7B-Baseæ¨¡å‹è¿›è¡Œæ¨ç†
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.generation import GenerationConfig

MODEL_NAME_OR_PATH = "qihoo360/360Zhinao3-7B"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME_OR_PATH, 
    trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True).cuda()

generation_config = GenerationConfig.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True)
generation_config.max_new_tokens = 1024

inputs = tokenizer('ä¸­å›½äºŒåå››èŠ‚æ°”\n1. ç«‹æ˜¥\n2. é›¨æ°´\n3. æƒŠè›°\n4. æ˜¥åˆ†\n5. æ¸…æ˜\n', return_tensors='pt')
inputs = inputs.to(model.device)

pred = model.generate(input_ids=inputs["input_ids"], generation_config=generation_config)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
```

### Instructæ¨¡å‹æ¨ç†

æ­¤ä»£ç æ¼”ç¤ºä½¿ç”¨ ğŸ¤—Transformers å¿«é€Ÿä½¿ç”¨ 360Zhinao3-7B-Instruct æ¨¡å‹è¿›è¡Œæ¨ç†
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.generation import GenerationConfig

MODEL_NAME_OR_PATH = "qihoo360/360Zhinao3-7B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True).cuda()

generation_config = GenerationConfig.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True)
generation_config.max_new_tokens = 2048

messages = []

#round-1
print(f"user: ç®€å•ä»‹ç»ä¸€ä¸‹åˆ˜å¾·å")
messages.append({"role": "user", "content": "ç®€å•ä»‹ç»ä¸€ä¸‹åˆ˜å¾·å"})
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
pred = model.generate(input_ids=input_ids, generation_config=generation_config)
response = tokenizer.decode(pred.cpu()[0][len(input_ids[0]):], skip_special_tokens=True)
messages.append({"role": "assistant", "content": response})
print(f"gpt: {response}")


#round-1
print(f"user: ä»–æœ‰ä»€ä¹ˆä»£è¡¨ä½œ?")
messages.append({"role": "user", "content": "ä»–æœ‰ä»€ä¹ˆä»£è¡¨ä½œ?"})
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
pred = model.generate(input_ids=input_ids, generation_config=generation_config)
response = tokenizer.decode(pred.cpu()[0][len(input_ids[0]):], skip_special_tokens=True)
messages.append({"role": "assistant", "content": response})
print(f"gpt: {response}")
```

### é•¿æ€ç»´é“¾æ¨¡å‹æ¨ç†
```python
import re
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.generation import GenerationConfig

MODEL_NAME_OR_PATH = "qihoo360/360Zhinao3-7B-O1.5"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True).cuda()

generation_config = GenerationConfig.from_pretrained(
    MODEL_NAME_OR_PATH,
    trust_remote_code=True)
generation_config.max_new_tokens = 2048


def extract_thinking_and_answer(input_string):
    thinking, answer = "", ""
    # æå–ç­”æ¡ˆ
    pattern_answer = r'.*</think>(.*)$'
    match_answer = re.search(pattern_answer, input_string, re.S)
    if match_answer:
        answer = match_answer.group(1)
    else:
        return thinking, input_string

    # æå–æ€è€ƒè¿‡ç¨‹
    pattern_thinking = r'<think>(.*?)</think>'
    match_thinking = re.search(pattern_thinking, input_string, re.S)
    if match_thinking:
        thinking = match_thinking.group(1)

    return thinking, answer


messages = []
messages.append({"role": "user", "content": "ç°æœ‰ä¸€ç¬¼å­ï¼Œé‡Œé¢æœ‰é¸¡å’Œå…”å­è‹¥å¹²åªï¼Œæ•°ä¸€æ•°ï¼Œå…±æœ‰å¤´14ä¸ªï¼Œè…¿38æ¡ï¼Œæ±‚é¸¡å’Œå…”å­å„æœ‰å¤šå°‘åªï¼Ÿ"})
input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
pred = model.generate(input_ids=input_ids, generation_config=generation_config)
response = tokenizer.decode(pred.cpu()[0][len(input_ids[0]):], skip_special_tokens=True)
thinking, answer = extract_thinking_and_answer(response)
messages.append({"role": "assistant", "content": answer, "reasoning_content": thinking})
print(json.dumps(messages, ensure_ascii=False, indent=4))
```

<br>

# æ¨¡å‹æ¨ç†
## æ¨¡å‹éƒ¨ç½²
### vLLMå®‰è£…ç¯å¢ƒ
å¦‚å¸Œæœ›éƒ¨ç½²åŠåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨ `vllm==0.6.0`ã€‚

å¦‚æœä½ ä½¿ç”¨**CUDA 12.1å’ŒPyTorch 2.1**ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…vLLMã€‚
```shell
pip install vllm==0.6.0
```

å¦åˆ™è¯·å‚è€ƒvLLMå®˜æ–¹çš„[å®‰è£…è¯´æ˜](https://docs.vllm.ai/en/latest/getting_started/installation.html)ã€‚

>å®‰è£…å®Œæˆåï¼Œè¿˜éœ€è¦ä»¥ä¸‹æ“ä½œ~
1. æŠŠvllm/zhinao.pyæ–‡ä»¶å¤åˆ¶åˆ°envç¯å¢ƒå¯¹åº”çš„vllm/model_executor/modelsç›®å½•ä¸‹ã€‚
2. ç„¶ååœ¨vllm/model_executor/models/\_\_init\_\_.pyæ–‡ä»¶å¢åŠ ä¸€è¡Œä»£ç 

    ```shell
    "ZhinaoForCausalLM": ("zhinao", "ZhinaoForCausalLM"),
    ```

### vLLMæœåŠ¡å¯åŠ¨

å¯åŠ¨æœåŠ¡
```shell
python -m vllm.entrypoints.openai.api_server \
    --model qihoo360/360Zhinao3-7B-O1.5 \
    --served-model-name 360Zhinao3-7B-O1.5 \
    --port 8360 \
    --host 0.0.0.0 \
    --dtype bfloat16 \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.8 \
    --trust-remote-code
```

ä½¿ç”¨curlè¯·æ±‚æœåŠ¡
```shell
curl http://localhost:8360/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
    "model": "360Zhinao3-7B-O1.5",
    "max_tokens": 200,
    "top_k": -1,
    "top_p": 0.8,
    "temperature": 1.0,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "ä½ å¥½"}
    ],
    "stop": [
        "<eod>",
        "<|im_end|>",
        "<|im_start|>"
    ]
}'

```

ä½¿ç”¨pythonè¯·æ±‚æœåŠ¡
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8360/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model="360Zhinao3-7B-O1.5",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "ä½ å¥½"},
    ],
    stop=[
        "<eod>",
        "<|im_end|>",
        "<|im_start|>"
    ],
    presence_penalty=0.0,
    frequency_penalty=0.0
)
print("Chat response:", chat_response)
```

> æ³¨æ„ï¼šå¦‚éœ€è¦å¼€å¯é‡å¤æƒ©ç½šï¼Œå»ºè®®ä½¿ç”¨ *presence_penalty* å’Œ *frequency_penalty* å‚æ•°ã€‚

<br>

# æ¨¡å‹å¾®è°ƒ
## è®­ç»ƒæ•°æ®

æˆ‘ä»¬æä¾›äº†å¾®è°ƒè®­ç»ƒæ ·ä¾‹æ•°æ® data/test.jsonï¼Œè¯¥æ ·ä¾‹æ•°æ®æ˜¯ä» [multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) é‡‡æ ·å‡º 1 ä¸‡æ¡ï¼Œå¹¶ä¸”åšäº†æ ¼å¼è½¬æ¢ã€‚

æ•°æ®æ ¼å¼:
```json
[
  {
    "id": 1,
    "conversations": [
        {
            "from": "system",
            "value": "You are a helpful assistant."
        },
        {
            "from": "user",
            "value": "æ‚¨å¥½å•Š"
        },
        {
            "from": "assistant",
            "value": "ä½ å¥½ï¼æˆ‘ä»Šå¤©èƒ½ä¸ºæ‚¨åšäº›ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆé—®é¢˜æˆ–éœ€è¦å¸®åŠ©å—? æˆ‘åœ¨è¿™é‡Œä¸ºæ‚¨æä¾›æœåŠ¡ã€‚"
        }
    ]
  }
]
```

## å¾®è°ƒè®­ç»ƒ
è®­ç»ƒè„šæœ¬å¦‚ä¸‹ï¼š
```shell
set -x

HOSTFILE=hostfile
DS_CONFIG=./finetune/ds_config_zero2.json

# PARAMS
LR=5e-6
EPOCHS=3
MAX_LEN=32768
BATCH_SIZE=4
NUM_NODES=1
NUM_GPUS=8
MASTER_PORT=29500

IS_CONCAT=False # æ˜¯å¦æ•°æ®æ‹¼æ¥åˆ°æœ€å¤§é•¿åº¦ï¼ˆMAX_LENï¼‰

DATA_PATH="./data/training_data_sample.json"
MODEL_PATH="qihoo360/360Zhinao3-7B"
OUTPUT_DIR="./outputs/"

deepspeed --hostfile ${HOSTFILE} \
        --master_port ${MASTER_PORT} \
        --num_nodes ${NUM_NODES} \
        --num_gpus ${NUM_GPUS} \
        finetune.py \
        --report_to "tensorboard" \
        --data_path ${DATA_PATH} \
        --model_name_or_path ${MODEL_PATH} \
        --output_dir ${OUTPUT_DIR} \
        --model_max_length ${MAX_LEN} \
        --num_train_epochs ${EPOCHS} \
        --per_device_train_batch_size ${BATCH_SIZE} \
        --gradient_accumulation_steps 1 \
        --save_strategy steps \
        --save_steps 200 \
        --learning_rate ${LR} \
        --lr_scheduler_type cosine \
        --adam_beta1 0.9 \
        --adam_beta2 0.95 \
        --adam_epsilon 1e-8 \
        --max_grad_norm 1.0 \
        --weight_decay 0.1 \
        --warmup_ratio 0.01 \
        --gradient_checkpointing True \
        --bf16 True \
        --tf32 True \
        --deepspeed ${DS_CONFIG} \
        --is_concat ${IS_CONCAT} \
        --logging_steps 1 \
        --log_on_each_node False
```
```shell
bash finetune/ds_finetune.sh
```
- å¯é€šè¿‡é…ç½®hostfileï¼Œå®ç°å•æœºã€å¤šæœºè®­ç»ƒã€‚
- å¯é€šè¿‡é…ç½®ds_configï¼Œå®ç°zero2ã€zero3ã€‚
- å¯é€šè¿‡é…ç½®fp16ã€bf16å®ç°æ··åˆç²¾åº¦è®­ç»ƒï¼Œå»ºè®®ä½¿ç”¨bf16ï¼Œä¸é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸€è‡´ã€‚
- å¯é€šè¿‡é…ç½®is_concatå‚æ•°ï¼Œæ§åˆ¶è®­ç»ƒæ•°æ®æ˜¯å¦æ‹¼æ¥ï¼Œå½“è®­ç»ƒæ•°æ®é‡çº§è¾ƒå¤§æ—¶ï¼Œå¯é€šè¿‡æ‹¼æ¥æå‡è®­ç»ƒæ•ˆç‡ã€‚

<br>

# è®¸å¯è¯

æœ¬ä»“åº“æºç éµå¾ªå¼€æºè®¸å¯è¯Apache 2.0ã€‚

360æ™ºè„‘å¼€æºæ¨¡å‹æ”¯æŒå…è´¹å•†ç”¨ï¼Œæ— éœ€å‘æˆ‘ä»¬è¿›è¡Œç‰¹æ®Šç”³è¯·ã€‚
